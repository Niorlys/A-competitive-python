
# Sorting and Searching

This topic is essential for competitive programming since it is usual that many problems become easier by sorting the input or transforming it. It turns out that some sorting algorithms can run in `O(n)` or `O(nlog(n))`. Typically, in problem-solving the task can be reduced to sorting the input and afterward making a search. For example, consider the task of determining the minimum non-negative difference between two elements of a given array. If we sort the array, all that remains is to search for the minimum difference between two consecutive elements of the array. 
 

There are many sorting algorithms out there, but facing problems with a high-level programming language like Python saves us from implementing them. However, the instrisic idea behind the design of certain algorithms like merge sort, can be useful, so it is worth it to take the time and learn some implementations. In Python, most of the time we'll use `sorted(arr, key=...)` or `_list.sort(key=...)` for sorting, the first when we want a new sorted instance of the list, and the latter for sorting in place that can perform faster with large lists. Binary search algorithms are convenient due to their `O(log(n))` complexity in already sorted arrays. Python has a module named `bisect` with a basic implementation of binary search implemented in C. However, we will use our implementations with different approaches, taking into account that binary search is a critical step when we need to do back verifications. Also, we take advantage of some solting algorithms and review the divide and conquer paradigm.

## Sorting and searching in problem-solving
In practice, the most common use of sorting and searching is to solve optimization problems, many of which can be solved by adopting a greedy approach where in each stage the best solution at the moment is considered without taking back the choice. The flag to check when we can use greedy based on sorting is determining the function `f(**kwargs)` we want to optimize, if the function is accumulative, then select two generic elements `x,y` and compare `f(x,y)` vs `f(y,x)` for `x<y`, otherwise just check if `f(x)` is monotone. Sorting the input of our problem sets an initial lower bound of `O(nlog(n))` (or maybe `O(n)`), usually, enough to pass the time limit check. We address our code into three main problems: 
 
### Sort for searching in `O(log(n))`
It is common to see a task that can be summarized as "Let `P` a proposition, find the maximum(minimum) value `x` of an array `A`, such that `P(x) == True`". This can be easily done by using a simple linear search checking out the proposition P for each value, but what if we have two arrays `A` and `B`, a proposition `P(a,b)`, and then for each `a in A` we need to find the optimal `b in B` such that `P(a,b) == True`? Assuming both arrays have the same length, if we adopt linear search, we get an `O(nÂ²)` algorithm, if we are solving a task like [this](https://cses.fi/problemset/task/1091) where the maximum length of arrays is `200K`, then our script would face running 40 billions of operations. However, by adopting a binary search approach we can do our entire search taking at most `4M` of operations.

**Searching for the smallest good value**
There is an intersting application of binary search for sorted arrays with an *inductive property*, which is a proposition `P` such that if `P(x)` is true, then `P(x+1)` is also true. In that case, we can find in `O(log(n))` the minimal value `m` such that `P(m)` is true. If we say that a value of the array is good when `P(x)` is true, then our task can be translated to find the smallest good value. The values can be elements from a given array or just the set of positive integers, the last case involves the necessity of an upper bound. The script includes an example using this approach in binary search. 
 
**Excluding items through marking and path compression**
It can happens in searching that once we have found the item, we need to erase it in order to ignore it in the next search. In Python, using `.pop(index)` operations is quite expensive if we are dealing with thousands of items. So we can opt to erase items abstractly instead as follows: If the item was found in index `i`, we mark it somehow that can be ignored in the next search checking if it marked or not. Then, we need to solve the task of given a index `i`, to determine the index `j<=i` that is not marked. It seems very easy using simple linear search, but in practice it is very inefficient. We can avoid linear search using path compression. To do this, we create a biyective map on indexes through a new array `mask` where `mask[i]=i`. An index is marked when we make it point to a previous index value. Using this technique, for an array of size `n`, we can see that for an index `k` if all the previous indexes are marked in descending order, the cost of updating `k` is `k+2`. So the worst case for an individual operation is at most `n+1` for the index `n-1`. Once the update is done, all subsequent operations are performed in `O(1)`. Thus, the amortized running time per operation is `O(1)`. The script implements and apply this technique.

### Sweep line: Finding maximum concurrency

 Given a list of `n` events, where each event is defined by a start time and an end time (both inclusive) during which it is active, we want to determine the maximum number of events that are active concurrently at any point in time. The problem encompasses many different scenarios like an airport where we know the arriving and leaving times of the planes or a set of intervals where we need to find the maximum number of intersected intervals. The script implements the solution for the last one.
  
### Choosing order to perform events
There are problems of optimization where we need to select an appropriate characteristic or parameter based on which to sort and get the required optimized value. For example, consider that we have `n` jobs, each job to be done in `t_i` time with a deadline of `d_i`, obtaining a profit `p_i=d_i-e_i`, where `e_i` is the moment when the task is ended. We want the order to execute the task that produces the maximum profit. If we take generics jobs `i,j`, and let `TP(i,j)` be the total profit when executing `i` before `j`, then `TP(i,j) = d_i-t_i + d_j -t_i-t_j =d_i+d_j - 2t_i-t_j`, and  `TP(j,i) = d_j-t_j + d_i -t_j-t_i =d_i+d_j - 2t_j-t_i`, so if `t_i<t_j` then `TP(i,j)>TP(j,i)`. This means that we must execute the jobs based on their duration `t_i`, so that is the parameter we use as a pivot to sort. Since the solution is so trivia, you can implement the solution by yourself.
