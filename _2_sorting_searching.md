
# Sorting and Searching
This topic is essential for competitive programming, as many problems become easier by sorting the input or transforming it. Some sorting algorithms can run in `O(n)` or `O(n log n)` time complexity. Typically, in problem-solving, the task can be reduced to sorting the input and then searching. For example, consider the task of determining the minimum non-negative difference between two elements of a given array. If we sort the array, all that remains is to search for the minimum difference between two consecutive elements of the array.

There are many sorting algorithms available, but using a high-level programming language like Python saves us from implementing them. However, understanding the intrinsic idea behind the design of certain algorithms, like merge sort, can be useful. It is worth taking the time to learn some implementations. In Python, most of the time we'll use `sorted(arr, key=...)` or `list.sort(key=...)` for sorting, the first when we want a new sorted instance of the list, and the latter for in-place sorting, which can perform faster with large lists. Binary search algorithms are convenient due to their `O(log n)` complexity in already sorted arrays. Python has a module named `bisect` with a basic implementation of binary search in C. However, we will use our implementations with different approaches, taking into account that binary search is a critical step when we need to do back verifications. Also, we take advantage of some sorting algorithms and review the Divide-Conquer paradigm.

### Sorting and Searching in Problem-Solving
In practice, the most common use of sorting and searching is to solve optimization problems, many of which can be solved by adopting a greedy approach. In each stage, the best solution at the moment is considered without retracting the choice. To check if we can use a greedy approach based on sorting, we determine if the function `f(**kwargs)` we want to optimize is accumulative. We select two generic elements `x` and `y` and compare `f(x, y)` vs `f(y, x)` for `x < y`. Otherwise, we just check if `f(x)` is monotone. Sorting the input of our problem sets an initial lower bound of `O(n log n)` (or maybe `O(n)`), which is usually enough to pass the time limit check. We address our code into three main problems:

### Sort for Searching in O(log n)
It is common to see a task that can be summarized as "Let `P` be a proposition, find the maximum (minimum) value x of an array `A` such that `P(x) == True`". This can be easily done using a simple linear search checking the proposition `P` for each value, but what if we have two arrays `A` and `B`, a proposition `P(a, b)`, and for each `a` in `A`, we need to find the optimal `b` in `B` such that `P(a, b) == True`? Assuming both arrays have the same length, if we adopt linear search, we get an `O(n²)` algorithm. If we are solving a task like this, where the maximum length of arrays is 200K, our script would face running 40 billion operations. However, by adopting a binary search approach, we can complete our entire search in at most 4M operations.

**Searching for the Smallest Good Value**

There is an interesting application of binary search for sorted arrays with an **inductive property**, which is a proposition `P` such that if `P(x)` is true, then `P(x+1)` is also true. In that case, we can find the minimal value `m` such that `P(m)` is true in `O(log n)`. If we say that a value of the array is good when `P(x)` is true, then our task can be translated to finding the smallest good value. The values can be elements from a given array or just the set of positive integers, the latter case involving the necessity of an upper bound. The script includes an example using this approach in binary search.

**Excluding Items Through Marking and Path Compression**

In searching, it can happen that once we have found the item, we need to erase it to ignore it in the next search. In Python, using `.pop(index)` operations is quite expensive if we are dealing with thousands of items. So we can opt to erase items abstractly instead as follows: If the item was found in index `i`, we mark it in some way to be ignored in the next search, checking if it is marked or not. Then, we need to solve the task of, given an index `i`, determining the index `j <= i` that is not marked. It seems very easy using simple linear search, but in practice, it is very inefficient. We can avoid linear search using path compression. To do this, we create a bijective map on indexes through a new array `mask` where `mask[i] = i`. An index is marked when we make it point to a previous index value. Using this technique, for an array of size `n`, we can see that for an index `k`, if all the previous indexes are marked in descending order, the cost of updating `k` is `k+2`. So the worst case for an individual operation is at most n+1 for the index `n-1`. Once the update is done, all subsequent operations are performed in `O(1)`. Thus, the amortized running time per operation is `O(1)`. The script implements and applies this technique.

### Sweep Line: Finding Maximum Concurrency
Given a list of `n` events, where each event is defined by a start time and an end time (both inclusive) during which it is active, we want to determine the maximum number of events that are active concurrently at any point in time. The problem encompasses many different scenarios like an airport where we know the arriving and leaving times of the planes or a set of intervals where we need to find the maximum number of intersected intervals. The script implements the solution for the last one.

### Choosing Order to Perform Events
There are optimization problems where we need to select an appropriate characteristic or parameter based on which to sort and get the required optimized value. For example, consider that we have `n` jobs, each job to be done in `tᵢ` time with a deadline of `dᵢ`, obtaining a profit `pᵢ = dᵢ - eᵢ`, where `eᵢ` is the moment when the task is ended. We want the order to execute the task that produces the maximum profit. If we take generic jobs `i` and `j`, and let `TP(i, j)` be the total profit when executing `i` before `j`, then `TP(i, j) = dᵢ - tᵢ + dⱼ - tᵢ - tⱼ = dᵢ + dⱼ - 2tᵢ - tⱼ`, and `TP(j, i) = dⱼ - tⱼ + dᵢ - tᵢ - tⱼ = dᵢ + dⱼ - 2tⱼ - tᵢ`. So if `tᵢ < tⱼ`, then `TP(i, j) > TP(j, i)`. This means that we must execute the jobs based on their duration `tᵢ`, so that is the parameter we use as a pivot to sort. Since the solution is so trivial, you can implement the solution by yourself.